{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The libraries using in the project are\n",
    "- opencv-python and opencv-contrib-python == 4.2.0\n",
    "- numpy == 1.19.5\n",
    "- os\n",
    "- torchvision == 0.8.2+cu101\n",
    "- torch == 1.7.1+cu101\n",
    "- sklearn == 0.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "def imshow(x: np.array):\n",
    "    cv.imshow('img',x)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1\n",
    "\n",
    "- Target extraction of curling rock is based on Hough Circles with multiple parameters to encapsulate all of them and reduced overlaping bbox to one\n",
    "- Extracted patches from target extraction are compared to the histogram of a base curling rock (red and yellow) based on that value if they pass a threshold are counted at inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base_path = \"test/Task1/\"\n",
    "files = os.listdir(base_path)\n",
    "files = [x for x in files if '.png' in x ]\n",
    "files = sorted(files, key=lambda x : int(x.split(\".\")[0]))\n",
    "\n",
    "def extract_targets(x: str, base_path: str,debug : bool = False) -> list:\n",
    "    targets = []\n",
    "    img = cv.imread(base_path+x,0)\n",
    "    cimg = cv.imread(base_path+x)\n",
    "    # First pass for hough circles to contain with these parameters\n",
    "    circles = cv.HoughCircles(img,cv.HOUGH_GRADIENT,1,20,\n",
    "                                param1=40,param2=28,minRadius=12,maxRadius=24)\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        for i in circles[0,:]:\n",
    "            x1,y1 = int(i[0])-int(i[2]),int(i[1])-int(i[2])\n",
    "            x2,y2 = int(i[0])+int(i[2]),int(i[1])+int(i[2])\n",
    "            # fail safe for bbox out of bounds\n",
    "            if x1 <0:\n",
    "                x1 = 0\n",
    "            if x2 <0:\n",
    "                x2 = 0\n",
    "            if y1 <0:\n",
    "                y1 = 0\n",
    "            if y2 <0:\n",
    "                y2 = 0\n",
    "            targets.append([x1,y1,x2-x1,y2-y1])\n",
    "            if debug:\n",
    "                cv.rectangle(cimg,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "    # Second pass for hough circles to contain with these parameters\n",
    "    img = cv.GaussianBlur(img,(5,5),0)\n",
    "    circles = cv.HoughCircles(img,cv.HOUGH_GRADIENT,1,20,\n",
    "                            param1=40,param2=28,minRadius=12,maxRadius=24)\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        for i in circles[0,:]:\n",
    "            x1,y1 = int(i[0])-int(i[2]),int(i[1])-int(i[2])\n",
    "            x2,y2 = int(i[0])+int(i[2]),int(i[1])+int(i[2])\n",
    "            if x1 <0:\n",
    "                x1 = 0\n",
    "            if x2 <0:\n",
    "                x2 = 0\n",
    "            if y1 <0:\n",
    "                y1 = 0\n",
    "            if y2 <0:\n",
    "                y2 = 0\n",
    "            targets.append([x1,y1,x2-x1,y2-y1])\n",
    "            if debug:\n",
    "                cv.rectangle(cimg,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "    # Last pass for hough circles to contain with these parameters\n",
    "    img = cv.imread(base_path+x,0)\n",
    "    circles = cv.HoughCircles(img,cv.HOUGH_GRADIENT,1,90,\n",
    "                                param1=35,param2=18,minRadius=15,maxRadius=25)\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        for i in circles[0,:]:\n",
    "            x1,y1 = int(i[0])-int(i[2]),int(i[1])-int(i[2])\n",
    "            x2,y2 = int(i[0])+int(i[2]),int(i[1])+int(i[2])\n",
    "            if x1 <0:\n",
    "                x1 = 0\n",
    "            if x2 <0:\n",
    "                x2 = 0\n",
    "            if y1 <0:\n",
    "                y1 = 0\n",
    "            if y2 <0:\n",
    "                y2 = 0\n",
    "            targets.append([x1,y1,x2-x1,y2-y1])\n",
    "            if debug:\n",
    "                cv.rectangle(cimg,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "    if debug:\n",
    "        imshow(cimg)\n",
    "    return cv.groupRectangles(targets,1,0.4)\n",
    "def calc_similarity(patch):\n",
    "    \n",
    "    ball1 = cv.imread('color1.png')\n",
    "    ball1 = cv.resize(ball1,(45,45))\n",
    "    ball2 = cv.imread('color2.png')\n",
    "    ball2 = cv.resize(ball2,(45,45))\n",
    "    \n",
    "    hist_base = cv.calcHist([patch], [0, 1, 2], None, [4, 4, 4],[0, 256, 0, 256, 0, 256])\n",
    "    hist_base = cv.normalize(hist_base, hist_base).flatten()\n",
    "\n",
    "    hist_1 = cv.calcHist([ball1], [0, 1, 2], None, [4, 4, 4],[0, 256, 0, 256, 0, 256])\n",
    "    hist_1 = cv.normalize(hist_1, hist_1).flatten()\n",
    "\n",
    "    hist_2 = cv.calcHist([ball2], [0, 1, 2], None, [4, 4, 4],[0, 256, 0, 256, 0, 256])\n",
    "    hist_2 = cv.normalize(hist_2, hist_2).flatten()\n",
    "\n",
    "    val1 = cv.compareHist(hist_base, hist_1, 0)\n",
    "    val2 = cv.compareHist(hist_base, hist_2, 0)\n",
    "    return val1,val2\n",
    "def infer_similarity(base_path: str, x: str) -> list:\n",
    "    yellow = 0 \n",
    "    red = 0\n",
    "    img = cv.imread(base_path+x)\n",
    "    for y in extract_targets(x,base_path,False)[0]:\n",
    "        #extract patch\n",
    "        image = img[y[1]:y[1]+y[3],y[0]:y[0]+y[2],:]\n",
    "        #calculate similarity between patch and curling ball1 and ball2\n",
    "        val1,val2 = calc_similarity(image)\n",
    "        if val1 > 0.50 or val2 > 0.50:\n",
    "            if val1 > 0.49:\n",
    "                yellow+=1\n",
    "            else:\n",
    "                red+=1\n",
    "    return [red+yellow,red,yellow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# For accuracy calculation\n",
    "correct = 0\n",
    "for x in files:\n",
    "    file = open(base_path+\"ground-truth/\"+x.replace(\".png\",'_gt.txt'))\n",
    "    out = [int(x.replace(\"\\n\",\" \")) for x in file.readlines()]\n",
    "    #print(out,infer_similarity(base_path,x))\n",
    "    correct+=out == infer_similarity(base_path,x)\n",
    "    file.close()\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions\n",
    "for idx,x in enumerate(files):\n",
    "    outstring = \" \".join(str(x) for x in infer_similarity(base_path,x)).replace(\" \",\"\\n\")\n",
    "    #print(outstring)\n",
    "    file = open('evaluation/submission_files/Dumitrascu_Claudiu_Cristian_407/Task1/'+str(idx+1)+\"_predicted.txt\",'w')\n",
    "    file.writelines(outstring)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2\n",
    "- For application we use the last frame from a video\n",
    "- Target extraction of curling rock is based on Hough Circles for the button part and expande it based on its radius\n",
    "- Extracted patches from target extraction are compared to the histogram of a base curling rock (red and yellow) based on that value if they pass a threshold are counted at inference and we take the bounding circles and see in which circle they are in, being sorted from button to the last circle, we use the function circle to see in which circle is present in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"test/Task2/\"\n",
    "files = os.listdir(base_path)\n",
    "files = [x for x in files if '.mp4' in x ]\n",
    "files = sorted(files, key=lambda x : int(x.split(\".\")[0]))\n",
    "\n",
    "def get_last_frame(video_path):\n",
    "    \"\"\"\n",
    "    This function takes the video path and returns the last frame.\n",
    "    :param video_path: Path to the video\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    cap = cv.VideoCapture(video_path)  \n",
    "    count = cap.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "    cap.set(cv.CAP_PROP_POS_FRAMES,count-1)\n",
    "    if cap.isOpened() == False: \n",
    "        raise Exception(\"Error opening video stream or file\") \n",
    "        return frames\n",
    "    while cap.isOpened():  \n",
    "        ret, frame = cap.read() # Read the frame\n",
    "        if ret is True:\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    return frames[0]\n",
    "\n",
    "def circle(x1, y1, x2, y2, r1, r2):\n",
    "    # calculate if two circle overlap of it contains it\n",
    "    distSq = (x1 - x2) * (x1 - x2) + (y1 - y2) * (y1 - y2);\n",
    "    radSumSq = (r1 + r2) * (r1 + r2);\n",
    "    if (distSq == radSumSq):\n",
    "        return 1\n",
    "    elif (distSq > radSumSq):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def infer_score(base_path: str, x: str, debug: bool = False) -> list:\n",
    "    # extract the button and then infer the rest of the circles\n",
    "    score = [0,0]\n",
    "    img = get_last_frame(base_path+x)\n",
    "    cv.imwrite(\"temp.png\",img)\n",
    "    img = cv.GaussianBlur(img,(5,5),0)\n",
    "    gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "    circles = cv.HoughCircles(gray,cv.HOUGH_GRADIENT,1,60,\n",
    "                                param1=35,param2=45,minRadius=50,maxRadius=135)\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        for i in circles[0,:]:\n",
    "            c1 = [i[0],i[1],i[2]//2-12]\n",
    "            c2 = [i[0],i[1],i[2]]\n",
    "            c3 = [i[0],i[1],2*i[2]-5]\n",
    "            c4 = [i[0],i[1],3*i[2]-10]\n",
    "            center = [i[0],i[1]]\n",
    "            if debug:\n",
    "                cv.circle(img,(i[0],i[1]),i[2]//2-12,(0,255,0),2)\n",
    "                cv.circle(img,(i[0],i[1]),i[2],(0,255,0),2)\n",
    "                cv.circle(img,(i[0],i[1]),2*i[2]-5,(0,255,0),2)\n",
    "                cv.circle(img,(i[0],i[1]),3*i[2]-10,(0,255,0),2)\n",
    "    rocks = extract_targets(\"temp.png\",'./')\n",
    "    # sort circles by distance from button center\n",
    "    for y in sorted(rocks[0],key=lambda x: np.sqrt((center[0] - x[0])**2 + (center[1] - x[1])**2)):        \n",
    "        image = img[y[1]:y[1]+y[3]-2,y[0]:y[0]+y[2]-2,:]\n",
    "        yellow = 0\n",
    "        red = 0\n",
    "        val1,val2 = calc_similarity(image)\n",
    "        was_button = 0\n",
    "        was_second = 0\n",
    "        # calculating score based on rules\n",
    "        if val1 > 0.50 or val2 > 0.50:\n",
    "            if val1 > 0.49:\n",
    "                yellow=1\n",
    "            else:\n",
    "                red=1\n",
    "            y_circ1,y_circ2,y_r = y[0]+y[2]//2 +1, y[1]+y[2]//2 +1, y[2]//2\n",
    "            first = circle(c4[0],c4[1],y_circ1,y_circ2,c4[2],y_r)\n",
    "            if first != -1:\n",
    "                button = circle(c1[0],c1[1],y_circ1,y_circ2,c1[2],y_r)\n",
    "                third = circle(c2[0],c2[1],y_circ1,y_circ2,c2[2],y_r)\n",
    "                second = circle(c3[0],c3[1],y_circ1,y_circ2,c3[2],y_r)\n",
    "                if button != -1:\n",
    "                    was_button = 1\n",
    "                    if yellow:\n",
    "                        score[0]+=1\n",
    "                    else:\n",
    "                        score[1]+=1\n",
    "                elif second != -1:\n",
    "                    was_second=1\n",
    "                    if yellow and score[1] == 0:\n",
    "                        score[0]+=1\n",
    "                    elif yellow and was_button == 0:\n",
    "                        score[0]= 1\n",
    "                        score[1]= 0\n",
    "                    elif score[0] ==0:\n",
    "                        score[1]+= 1\n",
    "                else:\n",
    "                    if yellow and score[1] == 0:\n",
    "                        score[0]+=1\n",
    "                    elif yellow and was_second == 0 and was_button == 0:\n",
    "                        score[0]= 1\n",
    "                        score[1]= 0\n",
    "                    elif score[0] ==0:\n",
    "                        score[1]+= 1\n",
    "                if debug:\n",
    "                    cv.circle(img,(y_circ1,y_circ2),y_r,(0,0,255),3)\n",
    "                    imshow(img)\n",
    "    score.reverse()\n",
    "    if debug:\n",
    "        imshow(img)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "correct = 0\n",
    "for x in files:\n",
    "    file = open(base_path+\"ground-truth/\"+x.replace(\".mp4\",'_gt.txt'))\n",
    "    out = [int(x.replace(\"\\n\",\" \")) for x in file.readlines()]\n",
    "    #print(out,infer_similarity(base_path,x))\n",
    "    correct+=out == infer_score(base_path,x)\n",
    "    file.close()\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "for idx,x in enumerate(files):\n",
    "    outstring = \" \".join(str(x) for x in infer_score(base_path,x)).replace(\" \",\"\\n\")\n",
    "    #print(outstring)\n",
    "    file = open('evaluation/submission_files/Dumitrascu_Claudiu_Cristian_407/Task2/'+str(idx+1)+\"_predicted.txt\",'w')\n",
    "    file.writelines(outstring)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "The method is based on CSRT tracker from Opencv in which takes the first bbox of the object and then is tracked along the way, in case of failing for the tracking, a new tracker is created with the curling rock extracted based on method 1 and then given as the new bbox for the tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0f761dd7c627>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mbounding_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mall_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[0mtracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".mp4\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0f761dd7c627>\u001b[0m in \u001b[0;36mget_all_frame\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Read the frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_path = \"test/Task3/\"\n",
    "files = os.listdir(base_path)\n",
    "files = [x for x in files if '.mp4' in x ]\n",
    "files = sorted(files, key=lambda x : int(x.split(\".\")[0]))\n",
    "\n",
    "def get_all_frame(video_path):\n",
    "    \"\"\"\n",
    "    This function takes the video path and returns the a list of frames.\n",
    "    :param video_path: Path to the video\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    cap = cv.VideoCapture(video_path)  \n",
    "    if cap.isOpened() == False: \n",
    "        raise Exception(\"Error opening video stream or file\") \n",
    "        return frames\n",
    "    while cap.isOpened():  \n",
    "        ret, frame = cap.read() # Read the frame\n",
    "        if ret is True:\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n",
    "    if interArea == 0:\n",
    "        return 0\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
    "    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "# tracker picker to test multiple \n",
    "def create_tracked(x=7):\n",
    "    tracker_types = ['BOOSTING', 'MIL','KCF', 'TLD', 'MEDIANFLOW', 'GOTURN', 'MOSSE', 'CSRT']\n",
    "    tracker_type = tracker_types[x]\n",
    "    if tracker_type == 'BOOSTING':\n",
    "        tracker = cv.TrackerBoosting_create()\n",
    "    if tracker_type == 'MIL':\n",
    "        tracker = cv.TrackerMIL_create()\n",
    "    if tracker_type == 'KCF':\n",
    "        tracker = cv.TrackerKCF_create()\n",
    "    if tracker_type == 'TLD':\n",
    "        tracker = cv.TrackerTLD_create()\n",
    "    if tracker_type == 'MEDIANFLOW':\n",
    "        tracker = cv.TrackerMedianFlow_create()\n",
    "    if tracker_type == 'GOTURN':\n",
    "        tracker = cv.TrackerGOTURN_create()\n",
    "    if tracker_type == 'MOSSE':\n",
    "        tracker = cv.TrackerMOSSE_create()\n",
    "    if tracker_type == \"CSRT\":\n",
    "        tracker = cv.TrackerCSRT_create()\n",
    "    return tracker\n",
    "correct = 0 \n",
    "predict = []\n",
    "for idx,x in enumerate(files):\n",
    "    bounding_box = []\n",
    "    debug=False\n",
    "    all_frames = get_all_frame(base_path+x)\n",
    "    tracking = open(base_path+x.replace(\".mp4\",'.txt'))\n",
    "    out = [x.replace(\"\\n\",\"\").split() for x in tracking.readlines()][-1]\n",
    "    out = [int(x) for x in out]\n",
    "    tracking.close()\n",
    "    tracker = create_tracked(7)\n",
    "    #init trackerde\n",
    "    ok = tracker.init(all_frames[0], (out[1],out[2],out[3]-out[1],out[4]-out[2]))\n",
    "    tracking.close()\n",
    "    bad = 0\n",
    "    for count,frame in enumerate(all_frames):\n",
    "        # Update tracker\n",
    "        ok, bbox = tracker.update(frame)\n",
    "        bbox = np.int32(np.round(bbox))\n",
    "\n",
    "        # Draw bounding box\n",
    "        if ok:\n",
    "            # Tracking success\n",
    "            p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            if debug:\n",
    "                cv.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "        else:\n",
    "            tracker = create_tracked(7)\n",
    "            #print('fail')\n",
    "            cv.imwrite(\"temp.png\",frame)\n",
    "            for y in extract_targets(\"temp.png\",'./',False)[0]:\n",
    "                image = frame[y[1]:y[1]+y[3],y[0]:y[0]+y[2],:]\n",
    "                val1,val2 = calc_similarity(image)\n",
    "                if val1 > 0.50 or val2 > 0.50:\n",
    "                    ok = tracker.init(frame, (y[0],y[1],y[2],y[3]))\n",
    "                    ok, bbox = tracker.update(frame)\n",
    "                    bbox = np.int32(np.round(bbox))\n",
    "                    # Draw bounding box\n",
    "                    if ok:\n",
    "                        # Tracking success\n",
    "                        p1 = (int(bbox[0]), int(bbox[1]))\n",
    "                        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "                        if debug:\n",
    "                            cv.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "                break\n",
    "        # Display result\n",
    "        if debug:\n",
    "            cv.imshow(\"Tracking\", frame)\n",
    "        predict = [count,bbox[0],bbox[1],bbox[0]+bbox[2],bbox[1]+bbox[3]]\n",
    "        bounding_box.append(predict)\n",
    "        if debug:\n",
    "            # Exit if ESC pressed\n",
    "            if count == len(all_frames)-3:\n",
    "                cv.waitKey(0)\n",
    "                cv.destroyAllWindows()\n",
    "                break\n",
    "            k = cv.waitKey(1) & 0xff\n",
    "            if k == 27 : \n",
    "                cv.destroyAllWindows()\n",
    "                break\n",
    "                \n",
    "                \n",
    "    # prediction writing\n",
    "    base_string = str(len(all_frames)) +\" -1 -1 -1 -1\\n\"\n",
    "    for x in bounding_box:\n",
    "        if x == bounding_box[-1]:\n",
    "            base_string+=str(x[0])+\" \"+str(x[1])+\" \"+str(x[2])+\" \"+str(x[3])+\" \"+str(x[4])\n",
    "        else:\n",
    "            base_string+=str(x[0])+\" \"+str(x[1])+\" \"+str(x[2])+\" \"+str(x[3])+\" \"+str(x[4])+\"\\n\" \n",
    "    file = open('evaluation/submission_files/Dumitrascu_Claudiu_Cristian_407/Task3/'+str(idx+1)+\"_predicted.txt\",'w')\n",
    "    file.writelines(base_string)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "This one is going to prove a little bit difficult, we have 10 files in which at the start of the frame the curling rock is not present and can appear any time so we cannot use something that we used previously the solution would be a pattern matching for mutiple views of the curling rock untill a part of the video and apply what we did for task 1,2 and 3 from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of frames 6802\n",
      "something wrong\n",
      "bad\n",
      "bad\n",
      "bad\n",
      "bad\n",
      "bad\n",
      "bad\n",
      "bad\n",
      "bad\n",
      "bad\n",
      "bad\n",
      "6802 0\n"
     ]
    }
   ],
   "source": [
    "base_path = \"test/Task4/\"\n",
    "files = os.listdir(base_path)\n",
    "files = [x for x in files if '.mp4' in x ]\n",
    "files = sorted(files, key=lambda x : int(x.split(\".\")[0]))\n",
    "\n",
    "total = 0\n",
    "frames = []\n",
    "labels = []\n",
    "for idx,x in enumerate(files):\n",
    "    all_frames = get_all_frame(base_path+x)\n",
    "    frames.append(all_frames)\n",
    "    total+=len(all_frames)\n",
    "    tracking = open('train/Task4/'+\"ground-truth/\"+x.replace(\".mp4\",'_gt.txt'))\n",
    "    out = [x.replace(\"\\n\",\"\").split() for x in tracking.readlines()]\n",
    "    out = [[int(y) for y in x ]for x in out]\n",
    "    labels.append(out)\n",
    "print(\"Total amount of frames\",total)\n",
    "\n",
    "if len(labels) == len(frames):\n",
    "    print(\"something wrong\")\n",
    "    \n",
    "new_labels = []\n",
    "no_object = [0,0,0,0,0]\n",
    "for i,label in enumerate(labels):\n",
    "    label = label[1:]\n",
    "    for idx,x in enumerate(frames[i]):\n",
    "        no_object = [0,0,0,0,0]\n",
    "        #print(label[idx])\n",
    "        if idx+1 == len(frames[i])-1:\n",
    "            break\n",
    "        if idx+1 > len(labels)-2:\n",
    "            label.insert(idx+1,no_object)\n",
    "        if label[idx+1][0] != idx+1:\n",
    "            no_object[0] = idx+1\n",
    "            label.insert(idx+1,no_object)\n",
    "    if len(frames[i]) == len(label):\n",
    "        print(\"good\")\n",
    "        new_labels.append(label)\n",
    "    else:\n",
    "        print(\"bad\")\n",
    "all_frames = []\n",
    "for x in frames:\n",
    "    for y in x:\n",
    "        all_frames.append(y)\n",
    "labels = []\n",
    "for x in new_labels:\n",
    "    for y in x:\n",
    "        labels.append(y)\n",
    "        \n",
    "print(len(all_frames),len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning model for object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# spliting for train,validation\n",
    "x_train,x_test,y_train,y_test = train_test_split(all_frames, np.zeros((6802,4)), test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataClass(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        \"\"\"\n",
    "        :param data: data list with features\n",
    "        :param labels: label list\n",
    "        :param transform: Applying preprocessing on the data\n",
    "        \"\"\"\n",
    "        self.df = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> (torch.tensor, torch.tensor):\n",
    "        # resize for memory limitation\n",
    "        sample =torch.tensor(cv.resize(self.df[idx],(0,0),fx=0.25,fy=0.25))\n",
    "        label = torch.Tensor(self.labels[idx][1:])\n",
    "        sample = sample.permute(2,1,0)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5441"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1361"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "train_loader = DataLoader(DataClass(x_train,torch.zeros((5441,5)),None),batch_size=16,shuffle=True)\n",
    "test_loader = DataLoader(DataClass(x_test,torch.zeros((1361,5)),None),batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# model based on inception_v3 and 2 fully connected with sigmoid output for coordinates\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.backbone = models.inception_v3(pretrained=True, aux_logits=False,transform_input =True)\n",
    "        self.linear1 = nn.Linear(1000,500)\n",
    "        self.linear2 = nn.Linear(500,4)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.relu(self.backbone(x))\n",
    "        #print(x)\n",
    "        x = F.dropout(x,0.2)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel().cuda()\n",
    "optim = torch.optim.Adam(model.parameters(),lr=1e-4) #optimizer\n",
    "loss = nn.L1Loss() #classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,train_loader, optim, loss,network):\n",
    "    batch_interval=50\n",
    "    for batch_idx,(x,y) in enumerate(train_loader):\n",
    "        # we do this step to transform labels of bbox to sigmoid values\n",
    "        y[:,0] = y[:,0]/ 1280\n",
    "        y[:,1] = y[:,1]/ 720\n",
    "        y[:,2] = y[:,2]/ 1280\n",
    "        y[:,3] = y[:,3]/ 720\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        output = network(x)\n",
    "\n",
    "        loss_value = loss(output, y)\n",
    "\n",
    "        loss_value.backward()\n",
    "\n",
    "        optim.step()\n",
    "        if batch_idx % batch_interval == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                               100. * batch_idx / len(train_loader), loss_value.item()))\n",
    "    print('Train Epoch: {} Length {} \\tLoss: {:.6f}'.format(epoch, len(train_loader), loss_value.item()))\n",
    "    \n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = torch.max(boxA[0], boxB[0])\n",
    "    yA = torch.max(boxA[1], boxB[1])\n",
    "    xB = torch.min(boxA[2], boxB[2])\n",
    "    yB = torch.min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    if xB - xA + 1  > 0 and yB - yA + 1 > 0:\n",
    "        interArea = (xB - xA + 1) *  (yB - yA + 1)\n",
    "    elif  xB - xA + 1 < 0 or yB - yA + 1 < 0:\n",
    "        interArea = torch.tensor(0)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the intersection area\n",
    "    iou = interArea / (boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def test(epoch,test_loader,network,loss):\n",
    "    mse= 0 \n",
    "    iou_score = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            target = target.cuda()\n",
    "            data = data.cuda()\n",
    "\n",
    "            outputs = network(data)\n",
    "            \n",
    "            test_loss = loss(outputs, target)\n",
    "            # we transform outputs to bbox coordinates and calculate if IOU is higher then 0.20\n",
    "            outputs[:,0] = outputs[:,0]* 1280\n",
    "            outputs[:,1] = outputs[:,1]* 720\n",
    "            outputs[:,2] = outputs[:,2]* 1280\n",
    "            outputs[:,3] = outputs[:,3]* 720\n",
    "            good=0\n",
    "            for bbox1,bbox2 in zip(target,outputs):\n",
    "                intersect = bb_intersection_over_union(bbox1,bbox2)\n",
    "                if intersect > 0.20:\n",
    "                    good+=1\n",
    "            mse += F.mse_loss(outputs,target)\n",
    "            iou_score +=good\n",
    "    print('Test Epoch: {} Length {} \\tLoss: {:.6f}'.format(epoch, len(test_loader), test_loss.item()))\n",
    "    print(\"MSE - \", mse)\n",
    "    print(\"IOU with score > 0.20 \",iou_score,\" total \",len(test_loader)*16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using as loss L1Loss which is good for anomalies like the curling rock not being present and optimizer Adam with lre 1e-4, the model was trained on approx 6000 frames with labels of the bounding box of the curling rock, some labels we're added to train the model to say if the curling rock is present with the normal label or with [0,0,0,0] if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4792 (0%)]\tLoss: 0.152071\n",
      "Train Epoch: 0 [800/4792 (17%)]\tLoss: 0.120412\n",
      "Train Epoch: 0 [1600/4792 (33%)]\tLoss: 0.028468\n",
      "Train Epoch: 0 [2400/4792 (50%)]\tLoss: 0.035401\n",
      "Train Epoch: 0 [3200/4792 (67%)]\tLoss: 0.073926\n",
      "Train Epoch: 0 [4000/4792 (83%)]\tLoss: 0.044028\n",
      "Train Epoch: 0 Length 300 \tLoss: 0.047535\n",
      "Test Epoch: 0 Length 75 \tLoss: 0.024249\n",
      "MSE -  tensor(645929.8750, device='cuda:0')\n",
      "IOU with score > 0.20  577  total  1200\n",
      "Train Epoch: 1 [0/4792 (0%)]\tLoss: 0.070599\n",
      "Train Epoch: 1 [800/4792 (17%)]\tLoss: 0.035974\n",
      "Train Epoch: 1 [1600/4792 (33%)]\tLoss: 0.033896\n",
      "Train Epoch: 1 [2400/4792 (50%)]\tLoss: 0.020244\n",
      "Train Epoch: 1 [3200/4792 (67%)]\tLoss: 0.032225\n",
      "Train Epoch: 1 [4000/4792 (83%)]\tLoss: 0.033271\n",
      "Train Epoch: 1 Length 300 \tLoss: 0.018565\n",
      "Test Epoch: 1 Length 75 \tLoss: 0.018074\n",
      "MSE -  tensor(566694.1250, device='cuda:0')\n",
      "IOU with score > 0.20  843  total  1200\n",
      "Train Epoch: 2 [0/4792 (0%)]\tLoss: 0.083284\n",
      "Train Epoch: 2 [800/4792 (17%)]\tLoss: 0.054400\n",
      "Train Epoch: 2 [1600/4792 (33%)]\tLoss: 0.020296\n",
      "Train Epoch: 2 [2400/4792 (50%)]\tLoss: 0.021251\n",
      "Train Epoch: 2 [3200/4792 (67%)]\tLoss: 0.019380\n",
      "Train Epoch: 2 [4000/4792 (83%)]\tLoss: 0.019201\n",
      "Train Epoch: 2 Length 300 \tLoss: 0.121511\n",
      "Test Epoch: 2 Length 75 \tLoss: 0.038285\n",
      "MSE -  tensor(225783.6562, device='cuda:0')\n",
      "IOU with score > 0.20  782  total  1200\n",
      "Train Epoch: 3 [0/4792 (0%)]\tLoss: 0.022257\n",
      "Train Epoch: 3 [800/4792 (17%)]\tLoss: 0.033522\n",
      "Train Epoch: 3 [1600/4792 (33%)]\tLoss: 0.018231\n",
      "Train Epoch: 3 [2400/4792 (50%)]\tLoss: 0.072098\n",
      "Train Epoch: 3 [3200/4792 (67%)]\tLoss: 0.021930\n",
      "Train Epoch: 3 [4000/4792 (83%)]\tLoss: 0.016605\n",
      "Train Epoch: 3 Length 300 \tLoss: 0.026148\n",
      "Test Epoch: 3 Length 75 \tLoss: 0.020408\n",
      "MSE -  tensor(257150.2500, device='cuda:0')\n",
      "IOU with score > 0.20  882  total  1200\n",
      "Train Epoch: 4 [0/4792 (0%)]\tLoss: 0.015990\n",
      "Train Epoch: 4 [800/4792 (17%)]\tLoss: 0.017615\n",
      "Train Epoch: 4 [1600/4792 (33%)]\tLoss: 0.025270\n",
      "Train Epoch: 4 [2400/4792 (50%)]\tLoss: 0.014403\n",
      "Train Epoch: 4 [3200/4792 (67%)]\tLoss: 0.019869\n",
      "Train Epoch: 4 [4000/4792 (83%)]\tLoss: 0.065429\n",
      "Train Epoch: 4 Length 300 \tLoss: 0.014671\n",
      "Test Epoch: 4 Length 75 \tLoss: 0.066369\n",
      "MSE -  tensor(303010.7188, device='cuda:0')\n",
      "IOU with score > 0.20  909  total  1200\n",
      "Train Epoch: 5 [0/4792 (0%)]\tLoss: 0.013102\n",
      "Train Epoch: 5 [800/4792 (17%)]\tLoss: 0.019932\n",
      "Train Epoch: 5 [1600/4792 (33%)]\tLoss: 0.019252\n",
      "Train Epoch: 5 [2400/4792 (50%)]\tLoss: 0.021691\n",
      "Train Epoch: 5 [3200/4792 (67%)]\tLoss: 0.014339\n",
      "Train Epoch: 5 [4000/4792 (83%)]\tLoss: 0.017072\n",
      "Train Epoch: 5 Length 300 \tLoss: 0.013981\n",
      "Test Epoch: 5 Length 75 \tLoss: 0.047489\n",
      "MSE -  tensor(171316.4062, device='cuda:0')\n",
      "IOU with score > 0.20  895  total  1200\n",
      "Train Epoch: 6 [0/4792 (0%)]\tLoss: 0.018107\n",
      "Train Epoch: 6 [800/4792 (17%)]\tLoss: 0.018435\n",
      "Train Epoch: 6 [1600/4792 (33%)]\tLoss: 0.019039\n",
      "Train Epoch: 6 [2400/4792 (50%)]\tLoss: 0.069821\n",
      "Train Epoch: 6 [3200/4792 (67%)]\tLoss: 0.012688\n",
      "Train Epoch: 6 [4000/4792 (83%)]\tLoss: 0.021613\n",
      "Train Epoch: 6 Length 300 \tLoss: 0.019713\n",
      "Test Epoch: 6 Length 75 \tLoss: 0.017330\n",
      "MSE -  tensor(1000125.1250, device='cuda:0')\n",
      "IOU with score > 0.20  944  total  1200\n",
      "Train Epoch: 7 [0/4792 (0%)]\tLoss: 0.018759\n",
      "Train Epoch: 7 [800/4792 (17%)]\tLoss: 0.016199\n",
      "Train Epoch: 7 [1600/4792 (33%)]\tLoss: 0.013614\n",
      "Train Epoch: 7 [2400/4792 (50%)]\tLoss: 0.046828\n",
      "Train Epoch: 7 [3200/4792 (67%)]\tLoss: 0.011278\n",
      "Train Epoch: 7 [4000/4792 (83%)]\tLoss: 0.015998\n",
      "Train Epoch: 7 Length 300 \tLoss: 0.010421\n",
      "Test Epoch: 7 Length 75 \tLoss: 0.024423\n",
      "MSE -  tensor(188951.6719, device='cuda:0')\n",
      "IOU with score > 0.20  1032  total  1200\n",
      "Train Epoch: 8 [0/4792 (0%)]\tLoss: 0.015286\n",
      "Train Epoch: 8 [800/4792 (17%)]\tLoss: 0.014589\n",
      "Train Epoch: 8 [1600/4792 (33%)]\tLoss: 0.014121\n",
      "Train Epoch: 8 [2400/4792 (50%)]\tLoss: 0.014948\n",
      "Train Epoch: 8 [3200/4792 (67%)]\tLoss: 0.023001\n",
      "Train Epoch: 8 [4000/4792 (83%)]\tLoss: 0.014832\n",
      "Train Epoch: 8 Length 300 \tLoss: 0.067596\n",
      "Test Epoch: 8 Length 75 \tLoss: 0.011992\n",
      "MSE -  tensor(135656.7031, device='cuda:0')\n",
      "IOU with score > 0.20  1041  total  1200\n",
      "Train Epoch: 9 [0/4792 (0%)]\tLoss: 0.015172\n",
      "Train Epoch: 9 [800/4792 (17%)]\tLoss: 0.013169\n",
      "Train Epoch: 9 [1600/4792 (33%)]\tLoss: 0.009200\n",
      "Train Epoch: 9 [2400/4792 (50%)]\tLoss: 0.009186\n",
      "Train Epoch: 9 [3200/4792 (67%)]\tLoss: 0.016864\n",
      "Train Epoch: 9 [4000/4792 (83%)]\tLoss: 0.021142\n",
      "Train Epoch: 9 Length 300 \tLoss: 0.022781\n",
      "Test Epoch: 9 Length 75 \tLoss: 0.024823\n",
      "MSE -  tensor(166772.5625, device='cuda:0')\n",
      "IOU with score > 0.20  1096  total  1200\n",
      "Train Epoch: 10 [0/4792 (0%)]\tLoss: 0.050260\n",
      "Train Epoch: 10 [800/4792 (17%)]\tLoss: 0.039615\n",
      "Train Epoch: 10 [1600/4792 (33%)]\tLoss: 0.047802\n",
      "Train Epoch: 10 [2400/4792 (50%)]\tLoss: 0.013276\n",
      "Train Epoch: 10 [3200/4792 (67%)]\tLoss: 0.011947\n",
      "Train Epoch: 10 [4000/4792 (83%)]\tLoss: 0.015613\n",
      "Train Epoch: 10 Length 300 \tLoss: 0.013341\n",
      "Test Epoch: 10 Length 75 \tLoss: 0.016237\n",
      "MSE -  tensor(151024.5000, device='cuda:0')\n",
      "IOU with score > 0.20  1047  total  1200\n",
      "Train Epoch: 11 [0/4792 (0%)]\tLoss: 0.012120\n",
      "Train Epoch: 11 [800/4792 (17%)]\tLoss: 0.011192\n",
      "Train Epoch: 11 [1600/4792 (33%)]\tLoss: 0.013505\n",
      "Train Epoch: 11 [2400/4792 (50%)]\tLoss: 0.011664\n",
      "Train Epoch: 11 [3200/4792 (67%)]\tLoss: 0.009817\n",
      "Train Epoch: 11 [4000/4792 (83%)]\tLoss: 0.007934\n",
      "Train Epoch: 11 Length 300 \tLoss: 0.019342\n",
      "Test Epoch: 11 Length 75 \tLoss: 0.014057\n",
      "MSE -  tensor(146904.0781, device='cuda:0')\n",
      "IOU with score > 0.20  1105  total  1200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for e in range(12):\n",
    "    train(e,train_loader,optim,loss, model)\n",
    "    test(e,test_loader,model,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a model based on inception_v3 for object detection on frames which achieves the accuracy 92% accuracy on IOU at least 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the model to have it for predictions\n",
    "#torch.save(model.state_dict(), \"./task4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the pretrained model\n",
    "model.load_state_dict(torch.load(\"./task4.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_pred = []\n",
    "with torch.no_grad():\n",
    "    for x in frames:\n",
    "        #print(x[:5][:5])\n",
    "        #print(y[:5])\n",
    "        pred = []\n",
    "        all_loader = DataLoader(DataClass(x, torch.zeros((len(x),5)),None),batch_size=16)\n",
    "        for idx,(data,target) in enumerate(all_loader):\n",
    "            data = data.cuda()\n",
    "            out = model(data)\n",
    "            out[:,0] = out[:,0]* 1280\n",
    "            out[:,1] = out[:,1]* 720\n",
    "            out[:,2] = out[:,2]* 1280\n",
    "            out[:,3] = out[:,3]* 720\n",
    "            out = torch.round(out).int().detach().cpu().numpy()\n",
    "            pred.append(out)\n",
    "        all_pred.append(np.vstack(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[576, 500, 722, 632],\n",
       "       [549, 508, 711, 651],\n",
       "       [548, 508, 705, 643],\n",
       "       [556, 518, 724, 651],\n",
       "       [563, 509, 711, 644],\n",
       "       [556, 508, 724, 651],\n",
       "       [553, 505, 728, 645],\n",
       "       [559, 515, 731, 651],\n",
       "       [558, 507, 728, 644],\n",
       "       [552, 509, 726, 654],\n",
       "       [558, 511, 728, 643],\n",
       "       [561, 518, 733, 653],\n",
       "       [556, 511, 718, 650],\n",
       "       [560, 509, 720, 641],\n",
       "       [564, 508, 718, 643]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred[0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "for idx,bbox in enumerate(all_pred):\n",
    "    base_string = str(len(bbox)) +\" -1 -1 -1 -1\\n\"\n",
    "    for i,x in enumerate(bbox):\n",
    "        #print(bbox[-1])\n",
    "        #print(x)\n",
    "        if sum(x) < 25:\n",
    "            continue\n",
    "        if x is bbox[0][-1]:\n",
    "            base_string+=str(i)+\" \"+str(x[0])+\" \"+str(x[1])+\" \"+str(x[2])+\" \"+str(x[3])\n",
    "        else:\n",
    "            base_string+=str(i)+\" \"+str(x[0])+\" \"+str(x[1])+\" \"+str(x[2])+\" \"+str(x[3])+\"\\n\" \n",
    "    file = open('evaluation/submission_files/Dumitrascu_Claudiu_Cristian_407/Task4/'+str(idx+1)+\"_predicted.txt\",'w')\n",
    "    file.writelines(base_string)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For vizualization\n",
    "\n",
    "for item1,item2 in zip(frames,all_pred):\n",
    "    for count,(x,y) in enumerate(zip(item1,item2)):\n",
    "        cv.rectangle(x, (y[0],y[1]), (y[2],y[3]), (255,0,0), 2, 1)    \n",
    "        #if debug:\n",
    "        cv.imshow(\"Tracking\", x)\n",
    "        #predict = [count,bbox[0],bbox[1],bbox[0]+bbox[2],bbox[1]+bbox[3]]\n",
    "        #bounding_box.append(predict)\n",
    "        #if debug:\n",
    "        # Exit if ESC pressed\n",
    "        if count == len(all_frames)-3:\n",
    "            cv.waitKey(0)\n",
    "            cv.destroyAllWindows()\n",
    "            break\n",
    "        k = cv.waitKey(1) & 0xff\n",
    "        if k == 27 : \n",
    "            cv.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
